# ğŸ›ï¸ Amharic E-commerce Data Extractor

## ğŸ“Œ Project Overview

The **Amharic E-commerce Data Extractor** is an end-to-end pipeline for transforming unstructured Telegram e-commerce posts (text and images) into structured, business-ready data using **Named Entity Recognition (NER)**. It extracts key entities â€” **Product**, **Price**, **Location**, **Brand**, **Size**, and **Contact** â€” to power EthioMart, a centralized Amharic e-commerce hub.

The project addresses the challenge of decentralized commerce on Telegram by scraping, processing, labeling, and analyzing messages from various channels. It includes fine-tuning and comparing transformer models for Amharic NER, model interpretability, and vendor analytics for micro-lending.

---

## ğŸ—‚ï¸ Project Structure

```bash
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                  # CI/CD & model evaluation workflows
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ scraping_config.yaml        # Telegram API config (IDs, target channels)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw scraped messages & images
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ cleaned.parquet         # Preprocessed data (OCR + text)
â”‚   â””â”€â”€ labeled/
â”‚       â”œâ”€â”€ raw_for_annotation/
â”‚       â”‚   â””â”€â”€ messages_for_manual_labeling.json
â”‚       â”œâ”€â”€ v1/
â”‚       â”‚   â”œâ”€â”€ train.conll
â”‚       â”‚   â”œâ”€â”€ val.conll
â”‚       â”‚   â””â”€â”€ test.conll
â”‚       â””â”€â”€ README.md               # Labeling rules and entity definitions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ *_finetuned/                # Fine-tuned model checkpoints (ignored by git)
â”‚   â””â”€â”€ model_cards/                # Evaluation results, model cards
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ vendor_scorecard.csv        # Vendor analytics output
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â””â”€â”€ vendor_scoring.py       # Vendor analytics & scorecard
â”‚   â”œâ”€â”€ data_pipeline/
â”‚   â”‚   â”œâ”€â”€ scraper.py              # Telegram scraper
â”‚   â”‚   â”œâ”€â”€ preprocessor.py         # OCR & cleaning logic
â”‚   â”‚   â””â”€â”€ labeling_tool.py        # Sampling & CoNLL conversion
â”‚   â”œâ”€â”€ ml_pipeline/
â”‚   â”‚   â”œâ”€â”€ training.py             # Model training script
â”‚   â”‚   â”œâ”€â”€ evaluation.py           # Model evaluation & comparison
â”‚   â”‚   â””â”€â”€ interpretability.py     # LIME-based model interpretability
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Task 1: Data Ingestion & Preprocessing

**Goal:** Scrape Telegram messages & images, perform OCR, and prepare unified text for NER.

**How to Run:**

1. Configure `configs/scraping_config.yaml` with your Telegram API credentials and channel list.
2. Scrape Telegram data:
   ```bash
   python src/data_pipeline/scraper.py
   ```
3. Run preprocessing (OCR + cleaning):
   ```bash
   python src/data_pipeline/preprocessor.py
   ```
   - Output: `data/processed/cleaned.parquet`

---

## âœ… Task 2: Manual Labeling & CoNLL Conversion

**Goal:** Create a high-quality, human-annotated NER dataset in CoNLL format with BIO tagging.

**How to Run:**

1. Sample messages for annotation:
   ```bash
   python src/data_pipeline/labeling_tool.py --action sample --num_samples 300
   ```
2. Annotate messages in `data/labeled/raw_for_annotation/messages_for_manual_labeling.json`.
3. Convert to CoNLL format:
   ```bash
   python src/data_pipeline/labeling_tool.py --action convert --annotated_file data/labeled/raw_for_annotation/messages_for_manual_labeling.json
   ```
   - Output: `train.conll`, `val.conll`, `test.conll`

---

## âœ… Task 3: Model Fine-Tuning

**Goal:** Fine-tune transformer models (mBERT, DistilBERT, XLM-R) for Amharic NER using the labeled CoNLL data.

**How to Run:**

```bash
python src/ml_pipeline/training.py
```

- Configure the model checkpoint in the script to switch between models.
- Outputs are saved in `models/*_finetuned/` (ignored by git).

---

## âœ… Task 4: Model Evaluation & Comparison

**Goal:** Evaluate and compare fine-tuned models on the test set using F1, precision, and recall. Select the best model for production.

**How to Run:**

```bash
python src/ml_pipeline/evaluation.py
```

- Outputs a comparison table and saves results to `models/model_cards/evaluation_results.csv`.

---

## âœ… Task 5: Model Interpretability (LIME)

**Goal:** Use LIME to explain which words most influence the model's prediction of entity presence in Amharic sentences.

**How to Run:**

```bash
python src/ml_pipeline/interpretability.py
```

- Prints word importances for each sample sentence and entity type.
- You can change the entity type in the script to analyze different entities.

---

## âœ… Task 6: Vendor Analytics & Scorecard

**Goal:** Combine NER results and message metadata to compute business metrics for each vendor/channel (posting frequency, average views, average price, top post, lending score).

**How to Run:**

```bash
python src/analytics/vendor_scoring.py
```

- Outputs a summary table and saves to `reports/vendor_scorecard.csv`.

---

## ğŸš€ Next Steps / Extending the Project

- Improve NER accuracy with more labeled data.
- Add more business metrics or visualizations.
- Integrate with a live dashboard or database.
- Expand interpretability to more entity types or use SHAP (with custom wrappers).

---

## ğŸ§© Dependencies

Install all dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ‘¥ Contributors

This project is built collaboratively for improving Ethiopian e-commerce accessibility and intelligence. For contribution guidelines and technical documentation, please refer to the respective module READMEs.

---

## ğŸ“„ License

Licensed under MIT. See `LICENSE` file for details.

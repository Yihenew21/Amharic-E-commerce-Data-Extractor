# ğŸ›ï¸ Amharic E-commerce Data Extractor

## ğŸ“Œ Project Overview

The **Amharic E-commerce Data Extractor** is a pipeline for transforming unstructured Telegram e-commerce posts (both text and images) into a structured format through **Named Entity Recognition (NER)**. It extracts key business entities â€” such as **Product**, **Price**, **Location**, **Brand**, **Size**, and **Contact** â€” and organizes them in a centralized format for **EthioMart**, an Amharic e-commerce hub.

The project addresses the challenge of decentralized commerce on Telegram by scraping, processing, and labeling messages from various channels. It lays the groundwork for fine-tuning **Large Language Models (LLMs)** specifically for Amharic NER tasks.

---

## ğŸ—‚ï¸ Project Structure

```bash
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                  # CI/CD pipelines (To be added)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ scraping_config.yaml        # Telegram API config (IDs, target channels)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ telegram/
â”‚   â”‚   â”‚   â””â”€â”€ [channel_name]/     # Per-channel scraped messages + images
â”‚   â”‚   â””â”€â”€ all_raw_telegram_messages.json
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ cleaned.parquet         # Preprocessed data (OCR + text)
â”‚   â””â”€â”€ labeled/
â”‚       â”œâ”€â”€ raw_for_annotation/
â”‚       â”‚   â””â”€â”€ messages_for_manual_labeling.json
â”‚       â”œâ”€â”€ v1/
â”‚       â”‚   â”œâ”€â”€ train.conll
â”‚       â”‚   â”œâ”€â”€ val.conll
â”‚       â”‚   â””â”€â”€ test.conll
â”‚       â””â”€â”€ README.md               # Labeling rules and entity definitions
â”œâ”€â”€ ml_pipeline/
â”‚   â”œâ”€â”€ training.py                 # Model training script (TBD)
â”‚   â””â”€â”€ evaluation.py               # Model evaluation script (TBD)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ data_pipeline/
â”‚       â”œâ”€â”€ scraper.py              # Telegram scraper
â”‚       â”œâ”€â”€ preprocessor.py         # OCR & cleaning logic
â”‚       â””â”€â”€ labeling_tool.py        # Sampling & CoNLL conversion
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                       # You're reading it
```

---

## âœ… Task 1: Data Ingestion & Preprocessing

### ğŸ¯ Objective

Build a robust ingestion system that:

- Scrapes Telegram messages & media (images),
- Performs OCR on images,
- Combines content for NER preprocessing.

### âš™ï¸ Components

#### ğŸ”¹ `scraper.py`

- Connects to Telegram using `Telethon`.
- Reads credentials and channels from `configs/scraping_config.yaml`.
- Downloads message metadata + image attachments.
- Outputs:
  - Raw per-channel data: `data/raw/telegram/[channel_name]/`
  - Unified JSON: `data/raw/all_raw_telegram_messages.json`

#### ğŸ”¹ `preprocessor.py`

- Loads raw messages.
- Cleans Amharic text (spacing, newline issues).
- Performs **OCR** on downloaded images using `Pytesseract` with `amh+eng` models.
- Merges original + OCR text into `combined_content_for_ner`.
- Outputs:
  - Preprocessed Parquet file: `data/processed/cleaned.parquet`

### ğŸš§ Challenges & Solutions

- **Telegram Auth:** Handled `Telethon` session persistence.
- **Image Management:** Stored local paths for images, handled naming conflicts.
- **OCR Accuracy:** Configured Tesseract to support Amharic (`amh.traineddata`) alongside English.

### â–¶ï¸ How to Run Task 1

1. **Configure**

   ```yaml
   # configs/scraping_config.yaml
   api_id: YOUR_API_ID
   api_hash: YOUR_API_HASH
   channels:
     - ecommerce_channel_1
     - ecommerce_channel_2
   ```

2. **Scrape Telegram Data**

   ```bash
   python src/data_pipeline/scraper.py
   ```

3. **Run Preprocessing (OCR + Cleaning)**
   ```bash
   python src/data_pipeline/preprocessor.py
   ```

---

## âœ… Task 2: Manual Labeling & CoNLL Conversion

### ğŸ¯ Objective

Create a high-quality, **human-annotated NER dataset** in **CoNLL** format with BIO tagging for fine-tuning a custom NER model.

### âš™ï¸ Components

#### ğŸ”¹ `labeling_tool.py`

- **Sampling Mode**

  ```bash
  python src/data_pipeline/labeling_tool.py --action sample --num_samples 300
  ```

  - Extracts 300 random entries from `cleaned.parquet`.
  - Saves to: `data/labeled/raw_for_annotation/messages_for_manual_labeling.json`

- **Annotation Format**

  ```json
  {
    "message_id": 12345,
    "combined_content_for_ner": "Sample Amharic message...",
    "annotations": [
      {
        "entity": "PRICE",
        "text": "1500 á‰¥áˆ­",
        "start": 10,
        "end": 17
      }
    ]
  }
  ```

- **Conversion Mode**
  ```bash
  python src/data_pipeline/labeling_tool.py --action convert --annotated_file data/labeled/raw_for_annotation/messages_for_manual_labeling.json
  ```
  - Converts annotated messages into token-level **BIO CoNLL format**.
  - Splits into: `train.conll`, `val.conll`, `test.conll`

#### ğŸ“˜ `data/labeled/README.md`

- **Entity Definitions:** Detailed labeling rules for each of the six target entities.
- **Guidance:** Examples and best practices for consistency across annotators.
- âš ï¸ **This file must be created and updated manually.**

### ğŸš§ Challenges & Solutions

- Designed simple, intuitive JSON structure for annotators.
- Wrote robust tokenizer-to-BIO converter to handle annotation alignment.
- Solved minor bugs (e.g., multi-line print syntax, file path mismatches).

---

### â–¶ï¸ How to Run Task 2

1. **Sample Messages**

   ```bash
   python src/data_pipeline/labeling_tool.py --action sample --num_samples 300
   ```

2. **Annotate Messages**

   - Open: `data/labeled/raw_for_annotation/messages_for_manual_labeling.json`
   - Add `annotations` for each message manually.

3. **Convert to CoNLL Format**

   ```bash
   python src/data_pipeline/labeling_tool.py --action convert --annotated_file data/labeled/raw_for_annotation/messages_for_manual_labeling.json
   ```

4. **Write Labeling Guidelines**
   - Add instructions to: `data/labeled/README.md`

---

## ğŸš€ Next Steps: Task 3 â€“ Model Fine-Tuning

With the pipeline and labeled data ready, the project is now set to move into **Task 3: Fine-tuning the NER model** using the prepared CoNLL data. This includes training scripts, hyperparameter tuning, and evaluation, to be implemented in the `ml_pipeline/` directory.

---

## ğŸ§© Dependencies

Install project dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ‘¥ Contributors

This project is built collaboratively for improving Ethiopian e-commerce accessibility and intelligence. For contribution guidelines and technical documentation, please refer to the respective module READMEs.

---

## ğŸ“„ License

Licensed under MIT. See `LICENSE` file for details.
